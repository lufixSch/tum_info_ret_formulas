\section{Math Basics}
\begin{mdframed}[style=redbox]
  i.i.d: independent and identically distributed
\end{mdframed}
\begin{mdframed}[style=eqbox]
\subsubsection*{Eigenvectors: \(A x = \lambda x\)}
A matrix $A \in \mathcal R^n$ has eigenvectors if $A$ is square and not singular (\(\det (A) \neq 0\)).
\vspace*{-4pt}\begin{align*}
  A = A^\top \implies \lambda_i \mathrm{~is~real~} && \operatorname*{rank}(A) = n \implies \lambda_i \neq 0 \quad \forall i\\
  x^\top A x > 0 \implies \lambda_i > 0 && x^\top A x \geq 0 \implies \lambda_i \geq 0 \quad \forall i
\end{align*}\vspace*{-16pt}\\
\textbf{Positive definite:} \(x^\top A x > 0 \quad \forall x \neq 0\)\\
\textbf{Positive semidefinite:} \(x^\top A x \geq 0 \quad \forall x \neq 0\)\\
\textbf{Jacoobi-Matrix:} \(\mat J_f(\mat x) = \cfrac{d f(\mat x)}{d\mat x} = \left[\cfrac{df_i(\mathbf x)}{dx_j}\right]_{i = 1\dots m; j = 1\dots n}\)\\
$g(x) \circ f(x) \implies \mat J_{g \circ f} (\mat x) = \mat J_g(f(\mat x)) \cdot \mat J_f(\mat x)$\\[0.25em]
$g(\mat x) = \mat W x \implies \mat J_{g \circ f} (\mat x) = \mat W \cdot \mat J_f(\mat x)$\\[0.35em]
\textbf{Hessian-Matrix:} \(\mat H_f(\mat x) = \cfrac{d^2 f(\mat x)}{d\mat x^2} = \left[\cfrac{\partial^2 f(\mat x)}{\partial x_j\partial x_k}\right]_\substack{i = 1\dots m;\\ j, k = 1\dots n}\)\\[-0.5em]
\small{Is symmetric - $\mat H_f(\mat x) = \mat J(\nabla f(\mat x))$}\\
\end{mdframed}
%
\begin{mdframed}[style=eqbox]
\subsection{Projection}
Given a subspace $\mathbb R^n \subset \mathbb R^p$ with the basis $\mat U$, the orthogonal projector onto $R^n$ is $\mat P = \mat U \mat U^\top$.
\vspace*{-6pt}\begin{align*}
  \mat P^2 = \mat P && \mat P \mat P^\top = \mat P^\top \mat P = \mat I
\end{align*}
\subsubsection{Orthogonality Principle ($\min \left[\mid\mid \underbar{y} - \mat{X} \underbar{t} \mid\mid^2\right]$)}
\vspace*{-4pt}
  \begin{align*}
    \mat{y} - \mat{X}\mat{t} \perp \text{range}[\mat{X}] &\implies \mat{y} - \mat{X}\mat{t} \in \text{ker}[\mat{X}^T]\\
    \mat{X}^T(\mat{y} - \mat{X}\mat{t}) = \mat{0} &\implies \mat{X}^T \mat{y} = \mat{X}^T \mat{X} \mat{t}\\
  \end{align*}\vspace*{-24pt}\\
  \small{if $N \geq \text{rank}[\mat{X}]$ (All columns of $\mat{X}$ are independent, $(\mat{X}^T \mat{X})^{-1}$ exists)}
\end{mdframed}
%
\begin{mdframed}[style=eqbox]
\subsection{Statistics}
\textbf{Normal Distribution:}
\(f_X(x) = \cfrac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\cfrac{(x-\mu)^2}{2\sigma^2}\right)\)
\textbf{Bernoulli Distribution:} \(f_X(x) = p^x (1 - p)^{1-x}\) \(x \in \{0, 1\}\)
\subsubsection*{Maximum likelihood estimation}
\vspace*{-10pt}\begin{align*}
  L(x;\theta) = \prod_{i=1}^N f_{X_i}(x_i; \theta) && l(x;\theta) = \sum_{i=1}^N \log f_{X_i}(x_i; \theta)
\end{align*}
%\subsubsection*{Bayes Estimation (Conditional Mean)}
%\vspace*{-4pt}\begin{align*}
%  T_{CM} : x &\mapsto E[\Theta \mid X = x] = \int \theta f_{\Theta \mid X}(\theta \mid x) \text{d}\theta\\
%  f_{\Theta \mid X}(\theta \mid x) &= \frac{f_{X \mid \Theta}(x \mid \theta) f_\Theta(\theta)}{f_X(x)}\\
%\end{align*}
\subsubsection*{Curse of dimensionality}
Given a random vector $\mat X \in \mathbb R^p$ with the $i$-th element $X_i$ i.i.d with $Pr(X_i^2 \leq \beta) \leq 1$
\vspace*{-4pt}\begin{align*}
Pr(\vert \vert X \vert\vert_2^2 \geq \beta ) \geq 1 - Pr(X_1^2 < \beta)^p
\end{align*}
In a $p$-dimensional space, $N^p$ samples are needed to achieve similar results as $N$ samples in a one-dimensional space.
\end{mdframed}
%
\begin{mdframed}[style=eqbox]
\subsection{Convexity}
\textbf{Convex set:} $\mathcal C$, $\mat x, \mat y \in \mathcal C, t \in [0,1]: t x + (1-t)y \in C$\\
\textbf{Convex function:} $f: C \to \mathbb R$, $f(t x + (1-t)y) \leq tf(x) + (1-t)f(y)$\\
\textbf{Concave function:} $g: C \to \mathbb R$, $g(t x + (1-t)y) \geq tg(x) + (1-t)g(y)$\\
If the Hessian is positive semidefinite e.g all entries $H_{ij} \geq 0$ (second derivative of a function is positive) the function is convex.
\subsubsection{Properties:}
Given two convex functions $f$ and $g$, the following functions are also convex:
\vspace*{-8pt}\begin{align*}
  h = \max (f, g) && h = f + g && h = g \circ f \text{ \small{if $g$ is non-decreasing}}
\end{align*}
\end{mdframed}
%
\begin{mdframed}[style=eqbox]
\subsection{Non-Linear Optimization}
\vspace*{-6pt}\begin{align*}
  \min_{\mat z} f(\mat z) &&\text{s.t.} \quad c_i(\mat z) = 0 \quad i \in \mathcal E && c_i(\mat z) \geq 0 \quad i \in \mathcal I
\end{align*}\vspace*{-14pt}\\
\textbf{Lagrange-function:} $L(\mat z, \mat \lambda) = f(\mat z) - \sum_i \lambda_i c_i(\mat z)$
\subsubsection{Karush-Kuhn-Tucker-Conditions}
\vspace*{-8pt}\begin{align*}
  \nabla_{\mat z} L(\mat z^*, \mat \lambda^*) = 0 && \lambda_i^* c_i(\mat z^*) = 0\\
  \lambda_i^* \geq 0 && c_i(\mat z^*) \geq 0 && \text{for } i \in \mathcal I\\
  && c_i(\mat z^*) = 0 && \text{for } i \in \mathcal E
\end{align*}
\subsubsection{Lagrangia Duality}
\vspace*{-6pt}\begin{align*}
  g(\mat \lambda) = \inf_{\mat z} L(\mat z, \mat \lambda) && \max_{\mat \lambda} g(\mat \lambda) \quad \text{s.t.} \quad \lambda_i \geq 0
\end{align*}
\end{mdframed}