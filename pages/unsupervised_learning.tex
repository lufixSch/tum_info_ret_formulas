\section{Unsupervised Learning}
\begin{mdframed}[style=eqbox]
\subsection{K-Means}
\vspace*{-6pt}\begin{align*}
  \mathcal F = \left\{f: \mathbb R^p \to \{\mat c_1, \ldots, \mat c_k\} \subset \mathbb R^p \right\} && L(\mat X, f(\mat X)) = \vert\vert \mat X - \mat f(\mat X) \vert\vert^2
\end{align*}\vspace*{-12pt}\\
\small{Where $\mathbb R^p$ is the feature space. $\mat c_i$ is the center of cluster $i$.}\\
Because $f$ maps to a finite set, the volume of the set distribution is zero.\\
\textbf{Algorithms:} Lloyd's algorithm, MaxQueen's algorithm
\end{mdframed}
%
\begin{mdframed}[style=eqbox]
\subsection{Principle Component Analysis}
\vspace*{-6pt}\begin{align*}
  \mathcal F = \left\{f: \mathbb R^p \to \mathbb R^k \subset \mathbb R^p \right\} && L(\mat X, f(\mat X)) = \vert\vert \mat X - \mat U_k \mat U_k^\top\mat X \vert\vert_2^2
\end{align*}\vspace*{-12pt}\\
\small{Where $\mathbb R^p$ is the feature space. And $f(\mat X) =\mat U_k^\top \mat X$ is a (orthogonal) projection on to the subspace $\mathbb R^k$.}\\
\textbf{Principle components:} $\mat S = \mat U_k^\top \mat X = \mat \Sigma_k \mat V_k^\top$\\
\textbf{New data:} $\mat s_i = \mat U_k^\top \mat y_i = \mat \Sigma_k^{-1} \mat V_k^\top \mat X^\top \mat y_i$
\subsubsection{Kernel PCA}
\vspace*{-10pt}\begin{align*}
  \tilde{\mat K} = \mat H \mat K \mat H = \mat V \mat \Sigma^2 \mat V^\top && \mat H = \mat I - \frac{1}{n} \mat{1}\mat{1}^\top && \mat S = \mat \Sigma \mat V^\top
\end{align*}
\vspace*{-16pt}\begin{align*}
\mat s_i = \mat \Sigma_k^{ - 1} \mat V_k^\top \mat k
  &&k = H \left([\kappa(x_1, y_i), \dots, \kappa(x_n, y_i)]^\top - \frac{1}{n}\mat K \mat 1\right)
\end{align*}
\end{mdframed}